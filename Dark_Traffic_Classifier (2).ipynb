{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dark Social Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this project is to build an effective __\"deep neural network\"__ that is capable of __classifying \"dark social\"__ web-traffic. First coined in 2012 by __Alexis Madrigal__, a journalist from __\"The Atlantic,\"__ the term is mostly used by marketers and business professionals to describe __website referral that is barely trackable__. The significant value of this issue is emphasized by one [study](https://radiumone.com/wp-content/uploads/2016/08/radiumone-the-dark-side-of-mobile-sharing-June-7-2016.pdf), which says that approximately __84% of website traffic in the world come through \"dark social\"__. On this, one of the most common form of dark social is __shared URL that is spreaded through email and instant messaging__. The afore-mentioned activity will be counted as \"direct traffic\" in most analytics programs nowadays, which provides deluted information and data for web-analyst.\n",
    "\n",
    "The following machine learning algorithm, utilizes the means of __\"natural language processing\"__ and further feed the processed data into a \"deep neural network\" in  order to teach a machine on how to identify \"dark social\" traffic by __analyzing URL composition__ that has been given to it. Due to the limitation of data available, the following algorithm is specifically designed to catch the most obvious \"dark social\" based on its URL formation, and leave out the ambigious URL as \"direct traffic\". Furthermore, from the result of its collection of data, the algorithm will be __mostly effective to be applied for e-commerce website__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __data_traffic_dataset.csv :__ Collection of website pages's URL from numbers of popular E-Commerce websites like Amazon, Alibaba, Lazada, Zalora, and Shoppee. Completely furnished with it dark social classification, the data reinforces a supervised machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "dataset = pd.read_csv('dark_traffic_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to clean url text\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def strip_url(text):\n",
    "    url = urlparse(text)\n",
    "    url = url._replace(scheme = ' ', query = ' ')\n",
    "    return url.geturl()\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub('[/+.?:]|-|=|html|ref|gp|www|co|id|com', ' ', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_url(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "# Set() are used to speed up the process of for loop\n",
    "# Stopwords contain words that are not necessary\n",
    "# Stemmer contain the process of reverting all words into its \n",
    "## original main form\n",
    "# Corpus is collection of text\n",
    "# Spartsity indicates the amount of sparmatrix within a data or \n",
    "## variable with very little amount\n",
    "# Tokenization is a process of put unique into different column\n",
    "# By cleaning out the text outside of CountVectorizer parametter,\n",
    "## you'll have more option\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = [] #still Empty\n",
    "for i in range (0, 915): # Modify this according to data size \n",
    "    review = denoise_text(dataset['Landing Page'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the bag of words model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Create the ANN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "# Unit = Amount of node in hidden layer ((Independent v. +1) /2)\n",
    "# Kernel_Initializer = Weight adjuster (close to 0)\n",
    "# activation = 'relu' (Reticfier)\n",
    "# input_dim = number of independent v.\n",
    "classifier.add(Dense(units = 890, kernel_initializer = 'uniform', activation = 'relu', input_dim = 1779))\n",
    "\n",
    "# Adding Second hidden layer\n",
    "classifier.add(Dense(units = 890, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding third hidden layer\n",
    "classifier.add(Dense(units = 890, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding output layer\n",
    "# For multiple dependent v. the needed activation function is softmax\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "# Optimizer + parameter which decides way to find the best weight. Adam is one of the best for stochastic algorithym\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test result\n",
    "Y_Pred = classifier.predict(X_test)\n",
    "Y_Pred = (Y_Pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, Y_Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
